From 08d542014d73f99c614fe040db3fa1e1549d4735 Mon Sep 17 00:00:00 2001
From: Jaehyun Hwang <jaehyun.hwang@cornell.edu>
Date: Tue, 8 Sep 2020 09:44:20 -0400
Subject: [PATCH] iosched: Add Disaggregated Storage I/O Scheduler (DSS)

Signed-off-by: Jaehyun Hwang <jaehyun.hwang@cornell.edu>
---
 Documentation/block/dss-iosched.rst |  22 ++
 block/Kconfig.iosched               |   8 +
 block/Makefile                      |   1 +
 block/dss-iosched.c                 | 421 ++++++++++++++++++++++++++++
 4 files changed, 452 insertions(+)
 create mode 100644 Documentation/block/dss-iosched.rst
 create mode 100644 block/dss-iosched.c

diff --git a/Documentation/block/dss-iosched.rst b/Documentation/block/dss-iosched.rst
new file mode 100644
index 000000000000..519920d00098
--- /dev/null
+++ b/Documentation/block/dss-iosched.rst
@@ -0,0 +1,22 @@
+==========================
+DSS I/O scheduler tunables
+==========================
+
+The three tunables for the DSS scheduler are the number of requests for
+reads/writes, the number of bytes for writes, and a timeout value.
+DSS will use these values for batching requests.
+
+batch_nr
+--------
+Number of requests for batching read/write requests
+Default: 16
+
+batch_bytes
+-----------
+Number of bytes for batching write requests
+Default: 65536 (bytes)
+
+batch_timeout
+-------------
+Timeout value for batching (in microseconds)
+Default: 50 (us)
diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 2f2158e05a91..6ccb76bae9bc 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -44,6 +44,14 @@ config BFQ_CGROUP_DEBUG
 	Enable some debugging help. Currently it exports additional stat
 	files in a cgroup which can be useful for debugging.
 
+config MQ_IOSCHED_DSS
+        tristate "DSS I/O scheduler"
+        default y
+        help
+          The Disaggregated Storage I/O Scheduler (DSS) supports batching
+          at BLK-MQ. Any device driver that benefits from batching
+          (e.g., NVMe-over-TCP) can use this scheduler.
+
 endmenu
 
 endif
diff --git a/block/Makefile b/block/Makefile
index 8d841f5f986f..eae108f2d2ab 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -21,6 +21,7 @@ obj-$(CONFIG_BLK_CGROUP_IOLATENCY)	+= blk-iolatency.o
 obj-$(CONFIG_BLK_CGROUP_IOCOST)	+= blk-iocost.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
+obj-$(CONFIG_MQ_IOSCHED_DSS)    += dss-iosched.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
 obj-$(CONFIG_IOSCHED_BFQ)	+= bfq.o
 
diff --git a/block/dss-iosched.c b/block/dss-iosched.c
new file mode 100644
index 000000000000..7d7f92b700d4
--- /dev/null
+++ b/block/dss-iosched.c
@@ -0,0 +1,421 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * The Disaggregated Storage I/O Scheduler (DSS) - supports batching
+ *      at blk-mq. The main use case is disaggregated storage access
+ *	using NVMe-over-Fabric (e.g., NVMe-over-TCP device driver).
+ *
+ * An early version of the idea is described and evaluated in
+ * "TCP â‰ˆ RDMA: CPU-efficient Remote Storage Access with i10",
+ * USENIX NSDI 2020.
+ *
+ * Copyright (C) 2020 Cornell University
+ *	Jaehyun Hwang <jaehyun.hwang@cornell.edu>
+ *	Qizhe Cai <qc228@cornell.edu>
+ *	Rachit Agarwal <ragarwal@cornell.edu>
+ */
+
+#include <linux/kernel.h>
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/elevator.h>
+#include <linux/module.h>
+#include <linux/sbitmap.h>
+
+#include "blk.h"
+#include "blk-mq.h"
+#include "blk-mq-debugfs.h"
+#include "blk-mq-sched.h"
+#include "blk-mq-tag.h"
+
+/* Default batch size in number of requests */
+#define DSS_BATCH_NR		16
+/* Default batch size in bytes (for write requests) */
+#define DSS_BATCH_BYTES		65536
+/* Default timeout value for batching (us units) */
+#define DSS_BATCH_TIMEOUT	50
+
+enum dss_state {
+	/* Batching state:
+	 * Do not run dispatching until we have
+	 * a certain amount of requests or a timer expires.
+	 */
+	DSS_STATE_BATCH = 0,
+
+	/* Dispatching state:
+	 * Run dispatching until all requests in the
+	 * scheduler's hctx queue are dispatched.
+	 */
+	DSS_STATE_DISPATCH,
+};
+
+struct dss_queue_data {
+	struct request_queue *q;
+
+	unsigned int	def_batch_nr;
+	unsigned int	def_batch_bytes;
+	unsigned int	def_batch_timeout;
+};
+
+struct dss_hctx_queue {
+	spinlock_t	lock;
+	struct		list_head rq_list;
+
+	struct blk_mq_hw_ctx	*hctx;
+
+	unsigned int	batch_nr;
+	unsigned int	batch_bytes;
+	unsigned int	batch_timeout;
+
+	unsigned int	qlen_nr;
+	unsigned int	qlen_bytes;
+
+	struct hrtimer	dss_timer;
+	enum dss_state	state;
+};
+
+static struct dss_queue_data *dss_queue_data_alloc(struct request_queue *q)
+{
+	struct dss_queue_data *qdata;
+
+	qdata = kzalloc_node(sizeof(*qdata), GFP_KERNEL, q->node);
+	if (!qdata)
+		return ERR_PTR(-ENOMEM);
+
+	qdata->q = q;
+	qdata->def_batch_nr = DSS_BATCH_NR;
+	qdata->def_batch_bytes = DSS_BATCH_BYTES;
+	qdata->def_batch_timeout = DSS_BATCH_TIMEOUT;
+
+	return qdata;
+}
+
+static int dss_init_sched(struct request_queue *q, struct elevator_type *e)
+{
+	struct dss_queue_data *qdata;
+	struct elevator_queue *eq;
+
+	eq = elevator_alloc(q, e);
+	if (!eq)
+		return -ENOMEM;
+
+	qdata = dss_queue_data_alloc(q);
+	if (IS_ERR(qdata)) {
+		kobject_put(&eq->kobj);
+		return PTR_ERR(qdata);
+	}
+
+	blk_stat_enable_accounting(q);
+
+	eq->elevator_data = qdata;
+	q->elevator = eq;
+
+	return 0;
+}
+
+static void dss_exit_sched(struct elevator_queue *e)
+{
+	struct dss_queue_data *qdata = e->elevator_data;
+
+	kfree(qdata);
+}
+
+enum hrtimer_restart dss_hctx_timeout_handler(struct hrtimer *timer)
+{
+	struct dss_hctx_queue *queue =
+		container_of(timer, struct dss_hctx_queue,
+			dss_timer);
+
+	queue->state = DSS_STATE_DISPATCH;
+	blk_mq_run_hw_queue(queue->hctx, true);
+
+	return HRTIMER_NORESTART;
+}
+
+static void dss_hctx_queue_reset(struct dss_hctx_queue *queue)
+{
+	queue->qlen_nr = 0;
+	queue->qlen_bytes = 0;
+	queue->state = DSS_STATE_BATCH;
+}
+
+static int dss_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+{
+	struct dss_hctx_queue *queue;
+
+	queue = kmalloc_node(sizeof(*queue), GFP_KERNEL, hctx->numa_node);
+	if (!queue)
+		return -ENOMEM;
+
+	spin_lock_init(&queue->lock);
+	INIT_LIST_HEAD(&queue->rq_list);
+
+	queue->hctx = hctx;
+	queue->batch_nr = 0;
+	queue->batch_bytes = 0;
+	queue->batch_timeout = 0;
+
+	hrtimer_init(&queue->dss_timer,
+		CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	queue->dss_timer.function = &dss_hctx_timeout_handler;
+
+	dss_hctx_queue_reset(queue);
+
+	hctx->sched_data = queue;
+
+	return 0;
+}
+
+static void dss_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	if (hrtimer_active(&queue->dss_timer))
+		hrtimer_cancel(&queue->dss_timer);
+	kfree(hctx->sched_data);
+}
+
+static bool dss_hctx_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio,
+		unsigned int nr_segs)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+	struct list_head *rq_list = &queue->rq_list;
+	bool merged;
+
+	spin_lock(&queue->lock);
+	merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio, nr_segs);
+	spin_unlock(&queue->lock);
+
+	if (merged && (bio->bi_opf & REQ_OP_MASK) == REQ_OP_WRITE)
+		queue->qlen_bytes += bio->bi_iter.bi_size;
+
+	return merged;
+}
+
+/*
+ * The batch size can be adjusted dynamically on a per-hctx basis. Use per-hctx
+ * variables in that case.
+ */
+static inline unsigned int dss_hctx_batch_nr(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_nr ?
+		queue->batch_nr : qdata->def_batch_nr;
+}
+
+static inline unsigned int dss_hctx_batch_bytes(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_bytes ?
+		queue->batch_bytes : qdata->def_batch_bytes;
+}
+
+static inline unsigned int dss_hctx_batch_timeout(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_timeout ?
+		queue->batch_timeout : qdata->def_batch_timeout;
+}
+
+static void dss_hctx_insert_update(struct dss_hctx_queue *queue,
+				struct request *rq)
+{
+	if ((rq->cmd_flags & REQ_OP_MASK) == REQ_OP_WRITE)
+		queue->qlen_bytes += blk_rq_bytes(rq);
+	queue->qlen_nr++;
+}
+
+static void dss_hctx_insert_requests(struct blk_mq_hw_ctx *hctx,
+				struct list_head *rq_list, bool at_head)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+	struct request *rq, *next;
+
+	list_for_each_entry_safe(rq, next, rq_list, queuelist) {
+		struct list_head *head = &queue->rq_list;
+
+		spin_lock(&queue->lock);
+		if (at_head)
+			list_move(&rq->queuelist, head);
+		else
+			list_move_tail(&rq->queuelist, head);
+		dss_hctx_insert_update(queue, rq);
+		blk_mq_sched_request_inserted(rq);
+		spin_unlock(&queue->lock);
+	}
+
+	/* Start a new timer */
+	if (queue->state == DSS_STATE_BATCH &&
+		!hrtimer_active(&queue->dss_timer))
+		hrtimer_start(&queue->dss_timer,
+			ns_to_ktime(dss_hctx_batch_timeout(hctx)
+				* NSEC_PER_USEC),
+			HRTIMER_MODE_REL);
+}
+
+static struct request *dss_hctx_dispatch_request(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+	struct request *rq;
+
+	spin_lock(&queue->lock);
+	rq = list_first_entry_or_null(&queue->rq_list,
+				struct request, queuelist);
+	if (rq)
+		list_del_init(&rq->queuelist);
+	else
+		dss_hctx_queue_reset(queue);
+	spin_unlock(&queue->lock);
+
+	return rq;
+}
+
+static inline bool dss_hctx_dispatch_now(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	return (queue->qlen_nr >= dss_hctx_batch_nr(hctx)) ||
+		(queue->qlen_bytes >= dss_hctx_batch_bytes(hctx));
+}
+
+/*
+ * Return true if we are in the dispatching state.
+ */
+static bool dss_hctx_has_work(struct blk_mq_hw_ctx *hctx)
+{
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	if (queue->state == DSS_STATE_BATCH) {
+		if (dss_hctx_dispatch_now(hctx)) {
+			queue->state = DSS_STATE_DISPATCH;
+			if (hrtimer_active(&queue->dss_timer))
+				hrtimer_cancel(&queue->dss_timer);
+		}
+	}
+
+	return (queue->state == DSS_STATE_DISPATCH);
+}
+
+#define DSS_DEF_BATCH_SHOW_STORE(name)					\
+static ssize_t dss_def_batch_##name##_show(struct elevator_queue *e,	\
+				char *page)				\
+{									\
+	struct dss_queue_data *qdata = e->elevator_data;		\
+									\
+	return sprintf(page, "%u\n", qdata->def_batch_##name);		\
+}									\
+									\
+static ssize_t dss_def_batch_##name##_store(struct elevator_queue *e,	\
+			const char *page, size_t count)			\
+{									\
+	struct dss_queue_data *qdata = e->elevator_data;		\
+	unsigned long long value;					\
+	int ret;							\
+									\
+	ret = kstrtoull(page, 10, &value);				\
+	if (ret)							\
+		return ret;						\
+									\
+	qdata->def_batch_##name = value;				\
+									\
+	return count;							\
+}
+DSS_DEF_BATCH_SHOW_STORE(nr);
+DSS_DEF_BATCH_SHOW_STORE(bytes);
+DSS_DEF_BATCH_SHOW_STORE(timeout);
+#undef DSS_DEF_BATCH_SHOW_STORE
+
+#define DSS_SCHED_ATTR(name)	\
+	__ATTR(batch_##name, 0644, dss_def_batch_##name##_show, dss_def_batch_##name##_store)
+static struct elv_fs_entry dss_sched_attrs[] = {
+	DSS_SCHED_ATTR(nr),
+	DSS_SCHED_ATTR(bytes),
+	DSS_SCHED_ATTR(timeout),
+	__ATTR_NULL
+};
+#undef DSS_SCHED_ATTR
+
+#ifdef CONFIG_BLK_DEBUG_FS
+#define DSS_DEBUGFS_SHOW(name)	\
+static int dss_hctx_batch_##name##_show(void *data, struct seq_file *m)	\
+{									\
+	struct blk_mq_hw_ctx *hctx = data;				\
+	struct dss_hctx_queue *queue = hctx->sched_data;		\
+									\
+	seq_printf(m, "%u\n", queue->batch_##name);			\
+	return 0;							\
+}									\
+									\
+static int dss_hctx_qlen_##name##_show(void *data, struct seq_file *m)	\
+{									\
+	struct blk_mq_hw_ctx *hctx = data;				\
+	struct dss_hctx_queue *queue = hctx->sched_data;		\
+									\
+	seq_printf(m, "%u\n", queue->qlen_##name);			\
+	return 0;							\
+}
+DSS_DEBUGFS_SHOW(nr);
+DSS_DEBUGFS_SHOW(bytes);
+#undef DSS_DEBUGFS_SHOW
+
+static int dss_hctx_state_show(void *data, struct seq_file *m)
+{
+	struct blk_mq_hw_ctx *hctx = data;
+	struct dss_hctx_queue *queue = hctx->sched_data;
+
+	seq_printf(m, "%d\n", queue->state);
+	return 0;
+}
+
+#define DSS_HCTX_QUEUE_ATTR(name)					\
+	{"batch_" #name, 0400, dss_hctx_batch_##name##_show},		\
+	{"qlen_" #name, 0400, dss_hctx_qlen_##name##_show}
+static const struct blk_mq_debugfs_attr dss_hctx_debugfs_attrs[] = {
+	DSS_HCTX_QUEUE_ATTR(nr),
+	DSS_HCTX_QUEUE_ATTR(bytes),
+	{"state", 0400, dss_hctx_state_show},
+	{},
+};
+#undef DSS_HCTX_QUEUE_ATTR
+#endif
+
+static struct elevator_type dss_sched = {
+	.ops = {
+		.init_sched = dss_init_sched,
+		.exit_sched = dss_exit_sched,
+		.init_hctx = dss_init_hctx,
+		.exit_hctx = dss_exit_hctx,
+		.bio_merge = dss_hctx_bio_merge,
+		.insert_requests = dss_hctx_insert_requests,
+		.dispatch_request = dss_hctx_dispatch_request,
+		.has_work = dss_hctx_has_work,
+	},
+#ifdef CONFIG_BLK_DEBUG_FS
+	.hctx_debugfs_attrs = dss_hctx_debugfs_attrs,
+#endif
+	.elevator_attrs = dss_sched_attrs,
+	.elevator_name = "dss",
+	.elevator_owner = THIS_MODULE,
+};
+
+static int __init dss_init(void)
+{
+	return elv_register(&dss_sched);
+}
+
+static void __exit dss_exit(void)
+{
+	elv_unregister(&dss_sched);
+}
+
+module_init(dss_init);
+module_exit(dss_exit);
+
+MODULE_AUTHOR("Jaehyun Hwang");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("DSS I/O scheduler");
-- 
2.22.0

