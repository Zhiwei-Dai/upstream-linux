From 36d5e17d16b94981f8342515b4f8df55c97ad837 Mon Sep 17 00:00:00 2001
From: Jaehyun Hwang <jaehyun.hwang@cornell.edu>
Date: Mon, 17 Aug 2020 09:41:33 -0400
Subject: [PATCH] iosched: Add SBQ (Stream Batch Queuing) I/O scheduler

Hi All,

We have several device drivers that benefit from batching requests at blk-mq,
such as mmc [1] and nvme-tcp [2,3]. Meanwhile, batching dispatch has been also supported
for I/O schedulers [4]. Thus the only remaining building block for realizing batching
is an I/O scheduler that enables these enhancements.

This patch introduces a new I/O scheduler, SBQ (Stream Batch Queuing), which
performs batching per hctx in terms of #requests, #bytes, and timeouts (at microseconds
granularity). SBQ starts dispatching only when #requests or #bytes is larger than
a default threshold or when a timer expires. After that, batching dispatch [3] would
happen, allowing batching at device drivers along with "bd->last" and ".commit_rqs".

We have tested the SBQ I/O scheduler with nvme-tcp optimizaitons [2,3] and batching
dispatch [4], varying number of cores, varying read/write ratios, and varying request
sizes, with NVMe SSD and RAM block device. SBQ shows ~1.6X improvements over "noop"
I/O scheduler in terms of IOPS with NVMe SSD. All results are available at [5].
(An early version of the idea is evaluated in [6].)

We currently use fixed default values as batching thresholds (e.g., 16 for #requests,
64KB for #bytes, and 50us for timeout based on sensitivity tests [6]), but plan to support
adaptive batching according to load dynamics. Furthermore, we also plan to extend
the scheduler to support multi-tenancy to improve both tail latency of latency-
sensitive applications and throughput of throughput-bound applications as a future work.

References
[1] https://lore.kernel.org/linux-block/cover.1587888520.git.baolin.wang7@gmail.com/T/#mc48a8fb6069843827458f5fea722e1179d32af2a
[2] https://git.infradead.org/nvme.git/commit/122e5b9f3d370ae11e1502d14ff5c7ea9b144a76
[3] https://git.infradead.org/nvme.git/commit/86f0348ace1510d7ac25124b096fb88a6ab45270
[4] https://lore.kernel.org/linux-block/20200630102501.2238972-1-ming.lei@redhat.com/
[5] https://github.com/i10-kernel/upstream-linux/blob/master/sbq-evaluation.pdf
[6] https://www.usenix.org/conference/nsdi20/presentation/hwang

Signed-off-by: Jaehyun Hwang <jaehyun.hwang@cornell.edu>
---
 Documentation/block/sbq-iosched.rst |  22 ++
 block/Kconfig.iosched               |   8 +
 block/Makefile                      |   1 +
 block/sbq-iosched.c                 | 421 ++++++++++++++++++++++++++++
 4 files changed, 452 insertions(+)
 create mode 100644 Documentation/block/sbq-iosched.rst
 create mode 100644 block/sbq-iosched.c

diff --git a/Documentation/block/sbq-iosched.rst b/Documentation/block/sbq-iosched.rst
new file mode 100644
index 000000000000..36af28f74a40
--- /dev/null
+++ b/Documentation/block/sbq-iosched.rst
@@ -0,0 +1,22 @@
+==========================
+SBQ I/O scheduler tunables
+==========================
+
+The three tunables for the SBQ scheduler are the number of requests for 
+reads/writes, the number of bytes for writes, and a timeout value.
+SBQ will use these values for batching requests.
+
+batch_nr
+--------
+Number of requests for batching read/write requests
+Default: 16
+
+batch_bytes
+-----------
+Number of bytes for batching write requests
+Default: 65536 (bytes)
+
+batch_timeout
+-------------
+Timeout value for batching (in microseconds)
+Default: 50 (us)
diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 2f2158e05a91..335816480eeb 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -44,6 +44,14 @@ config BFQ_CGROUP_DEBUG
 	Enable some debugging help. Currently it exports additional stat
 	files in a cgroup which can be useful for debugging.
 
+config MQ_IOSCHED_SBQ
+        tristate "SBQ I/O scheduler"
+        default y
+        help
+          The SBQ (Stream Batch Queuing) I/O scheduler supports batching
+          at BLK-MQ. Any device driver that benefits from batching
+          (e.g., NVMe-over-TCP) can use this scheduler.
+
 endmenu
 
 endif
diff --git a/block/Makefile b/block/Makefile
index 8d841f5f986f..368ea856f419 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -21,6 +21,7 @@ obj-$(CONFIG_BLK_CGROUP_IOLATENCY)	+= blk-iolatency.o
 obj-$(CONFIG_BLK_CGROUP_IOCOST)	+= blk-iocost.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
+obj-$(CONFIG_MQ_IOSCHED_SBQ)    += sbq-iosched.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
 obj-$(CONFIG_IOSCHED_BFQ)	+= bfq.o
 
diff --git a/block/sbq-iosched.c b/block/sbq-iosched.c
new file mode 100644
index 000000000000..44d5771241bd
--- /dev/null
+++ b/block/sbq-iosched.c
@@ -0,0 +1,421 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * The SBQ (Stream Batch Queuing) I/O scheduler - supports batching
+ *      at blk-mq. The main use case is NVMe-over-TCP device driver.
+ *
+ * An early version of the idea is described and evaluated in
+ * "TCP â‰ˆ RDMA: CPU-efficient Remote Storage Access with i10",
+ * USENIX NSDI 2020.
+ *
+ * Copyright (C) 2020 Cornell University
+ *	Jaehyun Hwang <jaehyun.hwang@cornell.edu>
+ *	Qizhe Cai <qc228@cornell.edu>
+ *	Ao Tang <atang@cornell.edu>
+ *	Rachit Agarwal <ragarwal@cornell.edu>
+ */
+
+#include <linux/kernel.h>
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/elevator.h>
+#include <linux/module.h>
+#include <linux/sbitmap.h>
+
+#include "blk.h"
+#include "blk-mq.h"
+#include "blk-mq-debugfs.h"
+#include "blk-mq-sched.h"
+#include "blk-mq-tag.h"
+
+/* Default batch size in number of requests */
+#define SBQ_BATCH_NR		16
+/* Default batch size in bytes (for write requests) */
+#define SBQ_BATCH_BYTES		65536
+/* Default timeout value for batching (us units) */
+#define SBQ_BATCH_TIMEOUT	50
+
+enum sbq_state {
+	/* Batching state:
+	 * Do not run dispatching until we have
+	 * a certain amount of requests or a timer expires.
+	 */
+	SBQ_STATE_BATCH = 0,
+
+	/* Dispatching state:
+	 * Run dispatching until all requests in the
+	 * scheduler's hctx queue are dispatched.
+	 */
+	SBQ_STATE_DISPATCH,
+};
+
+struct sbq_queue_data {
+	struct request_queue *q;
+
+	unsigned int	def_batch_nr;
+	unsigned int	def_batch_bytes;
+	unsigned int	def_batch_timeout;
+};
+
+struct sbq_hctx_queue {
+	spinlock_t	lock;
+	struct		list_head rq_list;
+
+	struct blk_mq_hw_ctx	*hctx;
+
+	unsigned int	batch_nr;
+	unsigned int	batch_bytes;
+	unsigned int	batch_timeout;
+
+	unsigned int	qlen_nr;
+	unsigned int	qlen_bytes;
+
+	struct hrtimer	sbq_timer;
+	enum sbq_state	state;
+};
+
+static struct sbq_queue_data *sbq_queue_data_alloc(struct request_queue *q)
+{
+	struct sbq_queue_data *qdata;
+
+	qdata = kzalloc_node(sizeof(*qdata), GFP_KERNEL, q->node);
+	if (!qdata)
+		return ERR_PTR(-ENOMEM);
+
+	qdata->q = q;
+	qdata->def_batch_nr = SBQ_BATCH_NR;
+	qdata->def_batch_bytes = SBQ_BATCH_BYTES;
+	qdata->def_batch_timeout = SBQ_BATCH_TIMEOUT;
+
+	return qdata;
+}
+
+static int sbq_init_sched(struct request_queue *q, struct elevator_type *e)
+{
+	struct sbq_queue_data *qdata;
+	struct elevator_queue *eq;
+
+	eq = elevator_alloc(q, e);
+	if (!eq)
+		return -ENOMEM;
+
+	qdata = sbq_queue_data_alloc(q);
+	if (IS_ERR(qdata)) {
+		kobject_put(&eq->kobj);
+		return PTR_ERR(qdata);
+	}
+
+	blk_stat_enable_accounting(q);
+
+	eq->elevator_data = qdata;
+	q->elevator = eq;
+
+	return 0;
+}
+
+static void sbq_exit_sched(struct elevator_queue *e)
+{
+	struct sbq_queue_data *qdata = e->elevator_data;
+
+	kfree(qdata);
+}
+
+enum hrtimer_restart sbq_hctx_timeout_handler(struct hrtimer *timer)
+{
+	struct sbq_hctx_queue *queue =
+		container_of(timer, struct sbq_hctx_queue,
+			sbq_timer);
+
+	queue->state = SBQ_STATE_DISPATCH;
+	blk_mq_run_hw_queue(queue->hctx, true);
+
+	return HRTIMER_NORESTART;
+}
+
+static void sbq_hctx_queue_reset(struct sbq_hctx_queue *queue)
+{
+	queue->qlen_nr = 0;
+	queue->qlen_bytes = 0;
+	queue->state = SBQ_STATE_BATCH;
+}
+
+static int sbq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+{
+	struct sbq_hctx_queue *queue;
+
+	queue = kmalloc_node(sizeof(*queue), GFP_KERNEL, hctx->numa_node);
+	if (!queue)
+		return -ENOMEM;
+
+	spin_lock_init(&queue->lock);
+	INIT_LIST_HEAD(&queue->rq_list);
+
+	queue->hctx = hctx;
+	queue->batch_nr = 0;
+	queue->batch_bytes = 0;
+	queue->batch_timeout = 0;
+
+	hrtimer_init(&queue->sbq_timer,
+		CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	queue->sbq_timer.function = &sbq_hctx_timeout_handler;
+
+	sbq_hctx_queue_reset(queue);
+
+	hctx->sched_data = queue;
+
+	return 0;
+}
+
+static void sbq_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	if (hrtimer_active(&queue->sbq_timer))
+		hrtimer_cancel(&queue->sbq_timer);
+	kfree(hctx->sched_data);
+}
+
+static bool sbq_hctx_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio,
+		unsigned int nr_segs)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+	struct list_head *rq_list = &queue->rq_list;
+	bool merged;
+
+	spin_lock(&queue->lock);
+	merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio, nr_segs);
+	spin_unlock(&queue->lock);
+
+	if (merged && (bio->bi_opf & REQ_OP_MASK) == REQ_OP_WRITE)
+		queue->qlen_bytes += bio->bi_iter.bi_size;
+
+	return merged;
+}
+
+/*
+ * The batch size can be adjusted dynamically on a per-hctx basis. Use per-hctx
+ * variables in that case.
+ */
+static inline unsigned int sbq_hctx_batch_nr(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_nr ?
+		queue->batch_nr : qdata->def_batch_nr;
+}
+
+static inline unsigned int sbq_hctx_batch_bytes(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_bytes ?
+		queue->batch_bytes : qdata->def_batch_bytes;
+}
+
+static inline unsigned int sbq_hctx_batch_timeout(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_queue_data *qdata = hctx->queue->elevator->elevator_data;
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	return queue->batch_timeout ?
+		queue->batch_timeout : qdata->def_batch_timeout;
+}
+
+static void sbq_hctx_insert_update(struct sbq_hctx_queue *queue,
+				struct request *rq)
+{
+	if ((rq->cmd_flags & REQ_OP_MASK) == REQ_OP_WRITE)
+		queue->qlen_bytes += blk_rq_bytes(rq);
+	queue->qlen_nr++;
+}
+
+static void sbq_hctx_insert_requests(struct blk_mq_hw_ctx *hctx,
+				struct list_head *rq_list, bool at_head)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+	struct request *rq, *next;
+
+	list_for_each_entry_safe(rq, next, rq_list, queuelist) {
+		struct list_head *head = &queue->rq_list;
+
+		spin_lock(&queue->lock);
+		if (at_head)
+			list_move(&rq->queuelist, head);
+		else
+			list_move_tail(&rq->queuelist, head);
+		sbq_hctx_insert_update(queue, rq);
+		blk_mq_sched_request_inserted(rq);
+		spin_unlock(&queue->lock);
+	}
+
+	/* Start a new timer */
+	if (queue->state == SBQ_STATE_BATCH &&
+		!hrtimer_active(&queue->sbq_timer))
+		hrtimer_start(&queue->sbq_timer,
+			ns_to_ktime(sbq_hctx_batch_timeout(hctx)
+				* NSEC_PER_USEC),
+			HRTIMER_MODE_REL);
+}
+
+static struct request *sbq_hctx_dispatch_request(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+	struct request *rq;
+
+	spin_lock(&queue->lock);
+	rq = list_first_entry_or_null(&queue->rq_list,
+				struct request, queuelist);
+	if (rq)
+		list_del_init(&rq->queuelist);
+	else
+		sbq_hctx_queue_reset(queue);
+	spin_unlock(&queue->lock);
+
+	return rq;
+}
+
+static inline bool sbq_hctx_dispatch_now(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	return (queue->qlen_nr >= sbq_hctx_batch_nr(hctx)) ||
+		(queue->qlen_bytes >= sbq_hctx_batch_bytes(hctx));
+}
+
+/*
+ * Return true if we are in the dispatching state.
+ */
+static bool sbq_hctx_has_work(struct blk_mq_hw_ctx *hctx)
+{
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	if (queue->state == SBQ_STATE_BATCH) {
+		if (sbq_hctx_dispatch_now(hctx)) {
+			queue->state = SBQ_STATE_DISPATCH;
+			if (hrtimer_active(&queue->sbq_timer))
+				hrtimer_cancel(&queue->sbq_timer);
+		}
+	}
+
+	return (queue->state == SBQ_STATE_DISPATCH);
+}
+
+#define SBQ_DEF_BATCH_SHOW_STORE(name)					\
+static ssize_t sbq_def_batch_##name##_show(struct elevator_queue *e,	\
+				char *page)				\
+{									\
+	struct sbq_queue_data *qdata = e->elevator_data;		\
+									\
+	return sprintf(page, "%u\n", qdata->def_batch_##name);		\
+}									\
+									\
+static ssize_t sbq_def_batch_##name##_store(struct elevator_queue *e,	\
+			const char *page, size_t count)			\
+{									\
+	struct sbq_queue_data *qdata = e->elevator_data;		\
+	unsigned long long value;					\
+	int ret;							\
+									\
+	ret = kstrtoull(page, 10, &value);				\
+	if (ret)							\
+		return ret;						\
+									\
+	qdata->def_batch_##name = value;				\
+									\
+	return count;							\
+}
+SBQ_DEF_BATCH_SHOW_STORE(nr);
+SBQ_DEF_BATCH_SHOW_STORE(bytes);
+SBQ_DEF_BATCH_SHOW_STORE(timeout);
+#undef SBQ_DEF_BATCH_SHOW_STORE
+
+#define SBQ_SCHED_ATTR(name)	\
+	__ATTR(batch_##name, 0644, sbq_def_batch_##name##_show, sbq_def_batch_##name##_store)
+static struct elv_fs_entry sbq_sched_attrs[] = {
+	SBQ_SCHED_ATTR(nr),
+	SBQ_SCHED_ATTR(bytes),
+	SBQ_SCHED_ATTR(timeout),
+	__ATTR_NULL
+};
+#undef SBQ_SCHED_ATTR
+
+#ifdef CONFIG_BLK_DEBUG_FS
+#define SBQ_DEBUGFS_SHOW(name)	\
+static int sbq_hctx_batch_##name##_show(void *data, struct seq_file *m)	\
+{									\
+	struct blk_mq_hw_ctx *hctx = data;				\
+	struct sbq_hctx_queue *queue = hctx->sched_data;		\
+									\
+	seq_printf(m, "%u\n", queue->batch_##name);			\
+	return 0;							\
+}									\
+									\
+static int sbq_hctx_qlen_##name##_show(void *data, struct seq_file *m)	\
+{									\
+	struct blk_mq_hw_ctx *hctx = data;				\
+	struct sbq_hctx_queue *queue = hctx->sched_data;		\
+									\
+	seq_printf(m, "%u\n", queue->qlen_##name);			\
+	return 0;							\
+}
+SBQ_DEBUGFS_SHOW(nr);
+SBQ_DEBUGFS_SHOW(bytes);
+#undef SBQ_DEBUGFS_SHOW
+
+static int sbq_hctx_state_show(void *data, struct seq_file *m)
+{
+	struct blk_mq_hw_ctx *hctx = data;
+	struct sbq_hctx_queue *queue = hctx->sched_data;
+
+	seq_printf(m, "%d\n", queue->state);
+	return 0;
+}
+
+#define SBQ_HCTX_QUEUE_ATTR(name)					\
+	{"batch_" #name, 0400, sbq_hctx_batch_##name##_show},		\
+	{"qlen_" #name, 0400, sbq_hctx_qlen_##name##_show}
+static const struct blk_mq_debugfs_attr sbq_hctx_debugfs_attrs[] = {
+	SBQ_HCTX_QUEUE_ATTR(nr),
+	SBQ_HCTX_QUEUE_ATTR(bytes),
+	{"state", 0400, sbq_hctx_state_show},
+	{},
+};
+#undef SBQ_HCTX_QUEUE_ATTR
+#endif
+
+static struct elevator_type sbq_sched = {
+	.ops = {
+		.init_sched = sbq_init_sched,
+		.exit_sched = sbq_exit_sched,
+		.init_hctx = sbq_init_hctx,
+		.exit_hctx = sbq_exit_hctx,
+		.bio_merge = sbq_hctx_bio_merge,
+		.insert_requests = sbq_hctx_insert_requests,
+		.dispatch_request = sbq_hctx_dispatch_request,
+		.has_work = sbq_hctx_has_work,
+	},
+#ifdef CONFIG_BLK_DEBUG_FS
+	.hctx_debugfs_attrs = sbq_hctx_debugfs_attrs,
+#endif
+	.elevator_attrs = sbq_sched_attrs,
+	.elevator_name = "sbq",
+	.elevator_owner = THIS_MODULE,
+};
+
+static int __init sbq_init(void)
+{
+	return elv_register(&sbq_sched);
+}
+
+static void __exit sbq_exit(void)
+{
+	elv_unregister(&sbq_sched);
+}
+
+module_init(sbq_init);
+module_exit(sbq_exit);
+
+MODULE_AUTHOR("Jaehyun Hwang");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("sbq I/O scheduler");
-- 
2.22.0

